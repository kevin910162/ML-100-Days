{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY_041  Decision Tree INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.資料是符合函數的假設，亦即沒有一對多的情形，這時候如果不對決策樹進行限制，決策樹是可以為每一個訓練樣本找到其對應的規則，就能夠把訓練 loss 降為0\n",
    "#### 2.Unlike other supervised learning algorithms, decision tree algorithm can be used for solving regression and classification problems too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions while creating Decision Tree :\n",
    "- At the beginning, the whole training set is considered as the root.\n",
    "- Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "- Records are distributed recursively on the basis of attribute values.\n",
    "- Order to placing attributes as root or internal node of the tree is done by using some statistical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees follow Sum of Product (SOP) representation,also known as Disjunctive Normal Form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using information gain as a criterion, we try to estimate the information contained by each attribute. We are going to use some points deducted from information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/IG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由於我們希望獲得的資訊量要最大，因此經由分割後的資訊量要越小越好。\n",
    "\n",
    "#### 常見的資訊量有兩種：熵(Entropy) 以及 Gini不純度(Gini Impurity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
